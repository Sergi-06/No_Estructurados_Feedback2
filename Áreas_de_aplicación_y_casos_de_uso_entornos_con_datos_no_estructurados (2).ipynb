{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# ACTIVIDAD : Redes Convolucionales\n",
        "\n",
        "---\n",
        "\n",
        "En esta actividad, vamos a trabajar con Convolutional Neural Networks para resolver un problema de clasificación de imágenes. En particular, vamos a clasificar imágenes de personajes los Simpsons.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/i8zIGqX.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consiste en imágenes de personajes de la serie extraídas directamente de capítulos de la serie. Este dataset ha sido recopilado por [Alexandre Attia](http://www.alexattia.fr/)\n",
        "Partiendo de los 18 personajes etiquetados, éstos pueden aparecer en distintas poses, en distintas posiciones de la imagen o con otros personajes en pantalla (si bien el personaje a clasificar siempre aparece en la posición predominante).\n",
        "\n",
        "El dataset de training puede ser descargado desde aquí:\n",
        "\n",
        "[Training data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60) (~500MB)\n",
        "\n",
        "Por otro lado, el dataset de test puede ser descargado de aquí:\n",
        "\n",
        "[Test data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8) (~10MB)\n",
        "\n",
        "Antes de empezar la práctica, se recomienda descargar las imágenes y echarlas un vistazo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D7tKOZ9BFfki"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sergi.zarzuelo.abel1\\AppData\\Local\\miniconda3\\envs\\no_estr_fed2_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "## Librerías utilizadas\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import kagglehub\n",
        "import shutil\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFJQpxdsrSKB"
      },
      "source": [
        "Nos descargamos la carpeta de la web de Kaggle:\n",
        "https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset?resource=download&select=simpsons_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xL42LoDCOdd",
        "outputId": "8f87c038-ea2c-4d10-f8e0-6f3d8ab3ba7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.12).\n",
            "Descargado en: C:\\Users\\sergi.zarzuelo.abel1\\.cache\\kagglehub\\datasets\\alexattia\\the-simpsons-characters-dataset\\versions\\4\n",
            "⚠️ Carpeta existente eliminada: ./simpsons_dataset\n",
            "✅ Dataset copiado a: ./simpsons_dataset\n"
          ]
        }
      ],
      "source": [
        "## Descargar dataset\n",
        "path = kagglehub.dataset_download(\"alexattia/the-simpsons-characters-dataset\")\n",
        "print(\"Descargado en:\", path)\n",
        "\n",
        "## Definir rutas\n",
        "source_path = os.path.join(path, \"simpsons_dataset\")\n",
        "target_path = \"./simpsons_dataset\"\n",
        "\n",
        "## Si ya existe, eliminar y copiar de nuevo\n",
        "if os.path.exists(target_path):\n",
        "    shutil.rmtree(target_path)\n",
        "    print(f\"⚠️ Carpeta existente eliminada: {target_path}\")\n",
        "\n",
        "shutil.copytree(source_path, target_path)\n",
        "print(f\"✅ Dataset copiado a: {target_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub7VfNIsst7G"
      },
      "source": [
        "Podemos ver que los personajes están repartidos en carpetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw0apA7ruy1c",
        "outputId": "daa583dd-0188-415b-d9cf-b187e8b65f12"
      },
      "outputs": [],
      "source": [
        "# ## Los personajes se encuentran en carpetas\n",
        "# !ls $path/simpsons_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrRrb_D0MuSI",
        "outputId": "d8f3d525-3624-4215-876d-cd6c460586df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simpsons_dataset               -> 0 imágenes\n",
            "lionel_hutz                    -> 3 imágenes\n",
            "disco_stu                      -> 8 imágenes\n",
            "troy_mcclure                   -> 8 imágenes\n",
            "miss_hoover                    -> 17 imágenes\n",
            "fat_tony                       -> 27 imágenes\n",
            "gil                            -> 27 imágenes\n",
            "otto_mann                      -> 32 imágenes\n",
            "sideshow_mel                   -> 40 imágenes\n",
            "agnes_skinner                  -> 42 imágenes\n",
            "rainier_wolfcastle             -> 45 imágenes\n",
            "cletus_spuckler                -> 47 imágenes\n",
            "snake_jailbird                 -> 55 imágenes\n",
            "professor_john_frink           -> 65 imágenes\n",
            "martin_prince                  -> 71 imágenes\n",
            "patty_bouvier                  -> 72 imágenes\n",
            "ralph_wiggum                   -> 89 imágenes\n",
            "carl_carlson                   -> 98 imágenes\n",
            "selma_bouvier                  -> 103 imágenes\n",
            "barney_gumble                  -> 106 imágenes\n",
            "groundskeeper_willie           -> 121 imágenes\n",
            "maggie_simpson                 -> 128 imágenes\n",
            "waylon_smithers                -> 181 imágenes\n",
            "mayor_quimby                   -> 246 imágenes\n",
            "lenny_leonard                  -> 310 imágenes\n",
            "nelson_muntz                   -> 358 imágenes\n",
            "edna_krabappel                 -> 457 imágenes\n",
            "comic_book_guy                 -> 469 imágenes\n",
            "kent_brockman                  -> 498 imágenes\n",
            "apu_nahasapeemapetilon         -> 623 imágenes\n",
            "sideshow_bob                   -> 877 imágenes\n",
            "abraham_grampa_simpson         -> 913 imágenes\n",
            "chief_wiggum                   -> 986 imágenes\n",
            "milhouse_van_houten            -> 1079 imágenes\n",
            "charles_montgomery_burns       -> 1193 imágenes\n",
            "principal_skinner              -> 1194 imágenes\n",
            "krusty_the_clown               -> 1206 imágenes\n",
            "marge_simpson                  -> 1291 imágenes\n",
            "bart_simpson                   -> 1342 imágenes\n",
            "lisa_simpson                   -> 1354 imágenes\n",
            "moe_szyslak                    -> 1452 imágenes\n",
            "ned_flanders                   -> 1454 imágenes\n",
            "homer_simpson                  -> 2246 imágenes\n"
          ]
        }
      ],
      "source": [
        "dataset_dir = target_path\n",
        "conteo_imagenes = {}\n",
        "\n",
        "for cls in sorted(os.listdir(dataset_dir)):\n",
        "    cls_path = os.path.join(dataset_dir, cls)\n",
        "    if os.path.isdir(cls_path):\n",
        "        imagenes = [f for f in os.listdir(cls_path) if f.endswith('.jpg')]\n",
        "        conteo_imagenes[cls] = len(imagenes)\n",
        "\n",
        "## Mostrar resultados ordenados\n",
        "for cls, count in sorted(conteo_imagenes.items(), key=lambda x: x[1]):\n",
        "    print(f\"{cls:30s} -> {count} imágenes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BmK1qaarpfc"
      },
      "source": [
        "Como Lionel solo tiene tres imágenes, lo quitamos del entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FJZLHp2_M745"
      },
      "outputs": [],
      "source": [
        "# !rm -r ./simpsons_dataset/lionel_hutz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree(\"./simpsons_dataset/lionel_hutz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrQNXod-sQVJ"
      },
      "source": [
        "Ahora generaremos diferentes carpetas separando los datos de train, test y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLrVzEcBK0O_",
        "outputId": "f6e7a428-0e4f-4ffa-b4d7-dd86bc18dd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Aviso] Clase 'simpsons_dataset' tiene muy pocas imágenes (0). Se asignan todas a entrenamiento.\n"
          ]
        }
      ],
      "source": [
        "## Definimos los paths\n",
        "ORIGINAL_DATASET_DIR = './simpsons_dataset'\n",
        "BASE_OUTPUT_DIR = './simpsons_split_dataset'\n",
        "\n",
        "## Porcentajes; he optado por generar sets de train, test y validación.\n",
        "## Pero podéis variar los porcentajes\n",
        "train_pct = 0.7\n",
        "val_pct = 0.15\n",
        "test_pct = 0.15\n",
        "\n",
        "## Crear estructura de carpetas\n",
        "splits = ['train', 'val', 'test']\n",
        "classes = os.listdir(ORIGINAL_DATASET_DIR)\n",
        "classes = [cls for cls in classes if os.path.isdir(os.path.join(ORIGINAL_DATASET_DIR, cls))]\n",
        "\n",
        "for split in splits:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(BASE_OUTPUT_DIR, split, cls), exist_ok=True)\n",
        "\n",
        "## Dividir y copiar imágenes\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(ORIGINAL_DATASET_DIR, cls)\n",
        "    images = [f for f in os.listdir(cls_path) if f.endswith('.jpg')]\n",
        "    random.shuffle(images)\n",
        "\n",
        "    # Asignación predeterminada\n",
        "    train, val, test = [], [], []\n",
        "\n",
        "    if len(images) >= 3:\n",
        "        train, temp = train_test_split(images, train_size=train_pct, random_state=42)\n",
        "        val, test = train_test_split(temp, test_size=test_pct / (test_pct + val_pct), random_state=42)\n",
        "    else:\n",
        "        # Si hay muy pocas imágenes, lo mandamos todo al entrenamiento\n",
        "        train = images\n",
        "        print(f\"[Aviso] Clase '{cls}' tiene muy pocas imágenes ({len(images)}). Se asignan todas a entrenamiento.\")\n",
        "\n",
        "    for img_list, split in zip([train, val, test], ['train', 'val', 'test']):\n",
        "        for img in img_list:\n",
        "            src = os.path.join(cls_path, img)\n",
        "            dst = os.path.join(BASE_OUTPUT_DIR, split, cls, img)\n",
        "            shutil.copyfile(src, dst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ykLYe16NQji",
        "outputId": "bfc56f30-3d0d-4f96-f04e-3b686f2207da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'abraham_grampa_simpson',\n",
              " 1: 'agnes_skinner',\n",
              " 2: 'apu_nahasapeemapetilon',\n",
              " 3: 'barney_gumble',\n",
              " 4: 'bart_simpson',\n",
              " 5: 'carl_carlson',\n",
              " 6: 'charles_montgomery_burns',\n",
              " 7: 'chief_wiggum',\n",
              " 8: 'cletus_spuckler',\n",
              " 9: 'comic_book_guy',\n",
              " 10: 'disco_stu',\n",
              " 11: 'edna_krabappel',\n",
              " 12: 'fat_tony',\n",
              " 13: 'gil',\n",
              " 14: 'groundskeeper_willie',\n",
              " 15: 'homer_simpson',\n",
              " 16: 'kent_brockman',\n",
              " 17: 'krusty_the_clown',\n",
              " 18: 'lenny_leonard',\n",
              " 19: 'lisa_simpson',\n",
              " 20: 'maggie_simpson',\n",
              " 21: 'marge_simpson',\n",
              " 22: 'martin_prince',\n",
              " 23: 'mayor_quimby',\n",
              " 24: 'milhouse_van_houten',\n",
              " 25: 'miss_hoover',\n",
              " 26: 'moe_szyslak',\n",
              " 27: 'ned_flanders',\n",
              " 28: 'nelson_muntz',\n",
              " 29: 'otto_mann',\n",
              " 30: 'patty_bouvier',\n",
              " 31: 'principal_skinner',\n",
              " 32: 'professor_john_frink',\n",
              " 33: 'rainier_wolfcastle',\n",
              " 34: 'ralph_wiggum',\n",
              " 35: 'selma_bouvier',\n",
              " 36: 'sideshow_bob',\n",
              " 37: 'sideshow_mel',\n",
              " 38: 'simpsons_dataset',\n",
              " 39: 'snake_jailbird',\n",
              " 40: 'troy_mcclure',\n",
              " 41: 'waylon_smithers'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Crear diccionario de mapeo automáticamente\n",
        "MAP_CHARACTERS = {i: cls for i, cls in enumerate(sorted(classes))}\n",
        "MAP_CHARACTERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Utilizando Convolutional Neural Networks, entrenar al menos dos clasificadores que sean capaz de reconocer personajes en imágenes de los Simpsons con una accuracy en el dataset de test de, al menos, **90%**. Redactar un informe analizando varias de las alternativas probadas y los resultados obtenidos.\n",
        "\n",
        "A continuación se detallan una serie de aspectos orientativos que podrían ser analizados en vuestro informe (no es necesario tratar todos ellos, pero cuánta más información podáis aportar mejor a la hora de desarrollar vuestro modelo):\n",
        "\n",
        "*   Análisis de los datos a utilizar. ¿Qué distribución siguen? ¿Están las clases balanceadas?\n",
        "*   Análisis de resultados, obtención de métricas de *precision* y *recall* por clase y análisis de qué clases obtienen mejores o peores resultados.\n",
        "*   Análisis visual de los errores de la red. ¿Qué tipo de imágenes o qué personajes dan más problemas a nuestro modelo?\n",
        "*   Comparación de modelos CNNs con un modelo de Fully Connected (sin convolución) para este problema.\n",
        "*   Utilización de distintas arquitecturas CNNs, comentando aspectos como su profundidad, hiperparámetros utilizados, optimizador, uso de técnicas de regularización, *batch normalization*, etc.\n",
        "*   Utilización de *data augmentation*. Esto puede conseguirse con la clase [ImageDataGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class) de Keras.\n",
        "\n",
        "\n",
        "Notas:\n",
        "* Los datos están en una única carpeta, por lo que tendrás que hacer el split entre train y test\n",
        "* Recuerda partir los datos en training/validation para tener una buena estimación de los valores que nuestro modelo tendrá en los datos de test, así como comprobar que no estamos cayendo en overfitting. Una posible partición puede ser 80 / 20.\n",
        "* No es necesario mostrar en el notebook las trazas de entrenamiento de todos los modelos entrenados, si bien una buena idea seria guardar gráficas de esos entrenamientos para el análisis. Sin embargo, **se debe mostrar el entrenamiento completo de al menos los dos mejores modelos obtenidos y la evaluación de los datos de test con estos modelos**.\n",
        "* Las imágenes **no están normalizadas**. Hay que normalizarlas como hemos hecho en trabajos anteriores.\n",
        "* El test set del problema tiene imágenes un poco más sencillas de identificar, por lo que es posible encontrarse con métricas en el test set bastante mejores que en el training set."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "no_estr_fed2_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
